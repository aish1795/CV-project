{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukJjmnTP5JiV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import ViTForImageClassification\n",
        "from transformers import ViTFeatureExtractor\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9gpg4HaG6o4"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "batch_size = 10\n",
        "lr = 0.00001\n",
        "gamma = 0.7\n",
        "seed = 42\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuoYsjW-4HW2"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ViTForImageClassification(nn.Module):\n",
        "    def __init__(self, num_labels=15):\n",
        "        super(ViTForImageClassification, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "    def forward(self, pixel_values, labels):\n",
        "        outputs = self.vit(pixel_values=pixel_values)\n",
        "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
        "        logits = self.classifier(output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        if loss is not None:\n",
        "            return logits, loss.item()\n",
        "        else:\n",
        "            return logits, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DwJrLCxuEVQ"
      },
      "outputs": [],
      "source": [
        "#train and test data directory\n",
        "data_dir = \"/scratch/sg6606/data\"\n",
        "TRANSFORM_IMG = {\n",
        "                'train':\n",
        "                transforms.Compose([\n",
        "                        transforms.Resize((256, 256)),\n",
        "                        transforms.RandomRotation(degrees=15),\n",
        "                        transforms.ColorJitter(contrast = (5)),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                ])\n",
        "                    }\n",
        "#load the train and test data\n",
        "dataset = ImageFolder(data_dir,transform=TRANSFORM_IMG['train'])\n",
        "print(len(dataset))\n",
        "train_size = int(6000)\n",
        "val_size = int(500)\n",
        "test_size = int(478)\n",
        "\n",
        "train_data, val_data, test_data = random_split(dataset,[train_size,val_size,test_size])\n",
        "print(f\"Length of Train Data : {len(train_data)}\")\n",
        "print(f\"Length of Validation Data : {len(val_data)}\")\n",
        "print(f\"Length of Test Data : {len(test_data)}\")\n",
        "\n",
        "train_dl = DataLoader(train_data, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
        "val_dl = DataLoader(val_data, batch_size, num_workers = 4, pin_memory = True)\n",
        "test_dl = DataLoader(test_data, batch_size*2, num_workers = 4, pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCCfzn0R6pdf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = ViTForImageClassification()    \n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAqGZS7X-ycN"
      },
      "outputs": [],
      "source": [
        "train_loss_array = []\n",
        "train_acc_array = []\n",
        "val_loss_array = []\n",
        "val_acc_array = []\n",
        "\n",
        "for epoch in range(epochs): \n",
        "    \n",
        "    epoch_accuracy=0\n",
        "    epoch_loss = 0\n",
        "    epoch_val_accuracy = 0\n",
        "    epoch_val_loss = 0\n",
        "    \n",
        "    for step, (x, y) in enumerate(train_dl):\n",
        "        x = np.split(np.squeeze(np.array(x)), batch_size)\n",
        "        for index, array in enumerate(x):\n",
        "            x[index] = np.squeeze(array)\n",
        "        x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
        "        x, y  = x.to(device), y.to(device)\n",
        "        b_x = Variable(x)   # batch x (image)\n",
        "        b_y = Variable(y)   # batch y (target)\n",
        "        output, loss = model(b_x, None)\n",
        "        if loss is None: \n",
        "            loss = loss_func(output, b_y)   \n",
        "            optimizer.zero_grad()           \n",
        "            loss.backward()                 \n",
        "            optimizer.step()\n",
        "        acc = (output.argmax(dim=1) == y).float().mean()\n",
        "        epoch_accuracy += acc / len(train_dl)\n",
        "        epoch_loss += loss / len(train_dl)\n",
        "    \n",
        "\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for val_x, val_y in val_dl:\n",
        "            val_x = np.split(np.squeeze(np.array(val_x)), batch_size)\n",
        "            for index, array in enumerate(val_x):\n",
        "                val_x[index] = np.squeeze(array)\n",
        "            val_x = torch.tensor(np.stack(feature_extractor(val_x)['pixel_values'], axis=0))\n",
        "            val_x = val_x.to(device)\n",
        "            val_y = val_y.to(device)\n",
        "            val_output, loss = model(val_x, val_y)\n",
        "            acc = (val_output.argmax(dim=1) == val_y).float().mean()\n",
        "            epoch_val_accuracy += acc / len(val_dl)\n",
        "            epoch_val_loss += loss / len(val_dl)\n",
        "    \n",
        "    train_loss_array.append(float(epoch_loss))\n",
        "    train_acc_array.append(float(epoch_accuracy))\n",
        "    val_loss_array.append(float(epoch_val_loss))\n",
        "    val_acc_array.append(float(epoch_val_accuracy))\n",
        "    \n",
        "    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\")\n",
        "\n",
        "print(train_loss_array)\n",
        "print(train_acc_array)\n",
        "print(val_loss_array)\n",
        "print(val_acc_array)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QJ_iisTDjWU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, epochs+1), train_acc_array, label='Training accuracy')\n",
        "plt.plot(range(1, epochs+1), val_acc_array, label='Validation accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, epochs+1), train_loss_array, label='Training loss')\n",
        "plt.plot(range(1, epochs+1), val_loss_array, label='Validation loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE1JmBVKDjWV"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "\tepoch_test_accuracy = 0\n",
        "\tepoch_test_loss = 0\n",
        "\tfor data, label in test_dl:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            test_output = model(data)\n",
        "            test_loss = criterion(val_output, label)\n",
        "\n",
        "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
        "            epoch_test_accuracy += acc / len(test_dl)\n",
        "            epoch_test_loss += val_loss / len(test_dl)\n",
        "\tprint(f\"Test accuracy: {epoch_test_accuracy:.4f}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "VIT-supervised (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}